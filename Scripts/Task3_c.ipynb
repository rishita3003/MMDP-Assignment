{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Part c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import librosa\n",
    "import librosa.display\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data folders\n",
    "ANTHEM_TEXT_DIR = \"national_anthems\"  # Folder containing anthem text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 255 country codes from C:\\Users\\Rishita\\Desktop\\MMDP-Assignment-change\\Task3_a\\countries.json\n",
      "\n",
      "Processing country 1: ad\n",
      "Country name: Andorra\n",
      "✓ Saved translation for Andorra\n",
      "Progress: 1/100\n",
      "\n",
      "Processing country 2: ae\n",
      "Country name: United Arab Emirates\n",
      "✓ Saved translation for United Arab Emirates\n",
      "Progress: 2/100\n",
      "\n",
      "Processing country 3: af\n",
      "Country name: Afghanistan\n",
      "✓ Saved translation for Afghanistan\n",
      "Progress: 3/100\n",
      "\n",
      "Processing country 4: ag\n",
      "Country name: Antigua and Barbuda\n",
      "✓ Saved translation for Antigua and Barbuda\n",
      "Progress: 4/100\n",
      "\n",
      "Processing country 5: ai\n",
      "Country name: Anguilla\n",
      "✓ Saved translation for Anguilla\n",
      "Progress: 5/100\n",
      "\n",
      "Processing country 6: al\n",
      "Country name: Albania\n",
      "✓ Saved translation for Albania\n",
      "Progress: 6/100\n",
      "\n",
      "Processing country 7: am\n",
      "Country name: Armenia\n",
      "✓ Saved translation for Armenia\n",
      "Progress: 7/100\n",
      "\n",
      "Processing country 8: ao\n",
      "Country name: Angola\n",
      "✓ Saved translation for Angola\n",
      "Progress: 8/100\n",
      "\n",
      "Processing country 9: aq\n",
      "Country name: Antarctica\n",
      "Error downloading translation for aq: 404 Client Error: Not Found for url: https://nationalanthems.info/aq.htm\n",
      "✗ Failed to get translation for Antarctica\n",
      "\n",
      "Processing country 10: ar\n",
      "Country name: Argentina\n",
      "✓ Saved translation for Argentina\n",
      "Progress: 9/100\n",
      "\n",
      "Processing country 11: as\n",
      "Country name: American Samoa\n",
      "✓ Saved translation for American Samoa\n",
      "Progress: 10/100\n",
      "\n",
      "Processing country 12: at\n",
      "Country name: Austria\n",
      "✓ Saved translation for Austria\n",
      "Progress: 11/100\n",
      "\n",
      "Processing country 13: au\n",
      "Country name: Australia\n",
      "✓ Saved translation for Australia\n",
      "Progress: 12/100\n",
      "\n",
      "Processing country 14: aw\n",
      "Country name: Aruba\n",
      "✓ Saved translation for Aruba\n",
      "Progress: 13/100\n",
      "\n",
      "Processing country 15: ax\n",
      "Country name: Åland Islands\n",
      "✓ Saved translation for Åland Islands\n",
      "Progress: 14/100\n",
      "\n",
      "Processing country 16: az\n",
      "Country name: Azerbaijan\n",
      "✓ Saved translation for Azerbaijan\n",
      "Progress: 15/100\n",
      "\n",
      "Processing country 17: ba\n",
      "Country name: Bosnia and Herzegovina\n",
      "✓ Saved translation for Bosnia and Herzegovina\n",
      "Progress: 16/100\n",
      "\n",
      "Processing country 18: bb\n",
      "Country name: Barbados\n",
      "✓ Saved translation for Barbados\n",
      "Progress: 17/100\n",
      "\n",
      "Processing country 19: bd\n",
      "Country name: Bangladesh\n",
      "✓ Saved translation for Bangladesh\n",
      "Progress: 18/100\n",
      "\n",
      "Processing country 20: be\n",
      "Country name: Belgium\n",
      "✓ Saved translation for Belgium\n",
      "Progress: 19/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 257\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed_countries\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# Example usage with a JSON file and a specific limit\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mRishita\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMMDP-Assignment-change\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTask3_a\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcountries.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 241\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(json_file_path, country_codes, subset_size)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProgress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(processed_countries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset_size\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39msubset_size\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(codes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# Be nice to the server\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Create metadata file\u001b[39;00m\n\u001b[0;32m    244\u001b[0m metadata_file \u001b[38;5;241m=\u001b[39m create_metadata(processed_countries)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create results directory\n",
    "RESULTS_DIR = \"anthem_analysis_results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "#################################################################\n",
    "# PART C: TEXTUAL ANALYSIS\n",
    "#################################################################\n",
    "\n",
    "def load_anthem_texts():\n",
    "    \"\"\"Load all anthem text files into a dictionary.\"\"\"\n",
    "    anthem_texts = {}\n",
    "    \n",
    "    for file_path in glob.glob(os.path.join(ANTHEM_TEXT_DIR, \"*.txt\")):\n",
    "        country_name = os.path.basename(file_path).replace('.txt', '')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                anthem_texts[country_name] = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(anthem_texts)} anthem text files.\")\n",
    "    return anthem_texts\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text: lowercase, remove punctuation, remove stopwords, lemmatize.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def analyze_anthem_texts(anthem_texts):\n",
    "    \"\"\"Perform textual analysis on anthem texts.\"\"\"\n",
    "    print(\"\\n--- TEXTUAL ANALYSIS ---\")\n",
    "    \n",
    "    # Preprocess texts\n",
    "    processed_texts = {country: preprocess_text(text) for country, text in anthem_texts.items()}\n",
    "    \n",
    "    # Basic statistics\n",
    "    text_lengths = {country: len(text.split()) for country, text in processed_texts.items()}\n",
    "    avg_length = np.mean(list(text_lengths.values()))\n",
    "    print(f\"Average anthem length (after preprocessing): {avg_length:.2f} words\")\n",
    "    \n",
    "    # Find shortest and longest anthems\n",
    "    shortest = min(text_lengths.items(), key=lambda x: x[1])\n",
    "    longest = max(text_lengths.items(), key=lambda x: x[1])\n",
    "    print(f\"Shortest anthem: {shortest[0]} ({shortest[1]} words)\")\n",
    "    print(f\"Longest anthem: {longest[0]} ({longest[1]} words)\")\n",
    "    \n",
    "    # Create a single corpus for overall analysis\n",
    "    all_text = ' '.join(processed_texts.values())\n",
    "    \n",
    "    # Most common words across all anthems\n",
    "    words = all_text.split()\n",
    "    word_counts = Counter(words)\n",
    "    print(\"\\nMost common words across all anthems:\")\n",
    "    for word, count in word_counts.most_common(15):\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    # Create word cloud for all anthems\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Most Common Words in National Anthems')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'anthem_wordcloud.png'))\n",
    "    \n",
    "    # TF-IDF Analysis\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform([text for text in processed_texts.values()])\n",
    "    \n",
    "    # Get feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Create DataFrame for easier analysis\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), \n",
    "                           index=list(processed_texts.keys()),\n",
    "                           columns=feature_names)\n",
    "    \n",
    "    # Save TF-IDF matrix\n",
    "    tfidf_df.to_csv(os.path.join(RESULTS_DIR, 'anthem_tfidf.csv'))\n",
    "    \n",
    "    # Dimensionality reduction for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "    # Create PCA plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)\n",
    "    \n",
    "    # Add country labels to points\n",
    "    for i, country in enumerate(processed_texts.keys()):\n",
    "        plt.annotate(country, (pca_result[i, 0], pca_result[i, 1]), \n",
    "                    fontsize=8, alpha=0.8)\n",
    "    \n",
    "    plt.title('PCA of National Anthem Texts')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'anthem_text_pca.png'))\n",
    "    \n",
    "    # Clustering analysis\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "    clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "    \n",
    "    # Create a DataFrame with clustering results\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'Country': list(processed_texts.keys()),\n",
    "        'Cluster': clusters\n",
    "    })\n",
    "    \n",
    "    # Print clusters\n",
    "    print(\"\\nAnthem text clusters:\")\n",
    "    for cluster_id in range(5):\n",
    "        countries = cluster_df[cluster_df['Cluster'] == cluster_id]['Country'].tolist()\n",
    "        print(f\"Cluster {cluster_id}: {', '.join(countries[:5])}{'...' if len(countries) > 5 else ''}\")\n",
    "    \n",
    "    # Save clustering results\n",
    "    cluster_df.to_csv(os.path.join(RESULTS_DIR, 'anthem_text_clusters.csv'), index=False)\n",
    "    \n",
    "    # Topic modeling with NMF\n",
    "    nmf = NMF(n_components=5, random_state=42)\n",
    "    nmf_result = nmf.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Get top words for each topic\n",
    "    print(\"\\nTop words for each topic:\")\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        top_words_idx = topic.argsort()[:-11:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "    \n",
    "    # Save topic modeling results\n",
    "    nmf_df = pd.DataFrame(nmf_result, index=list(processed_texts.keys()))\n",
    "    nmf_df.to_csv(os.path.join(RESULTS_DIR, 'anthem_topics.csv'))\n",
    "    \n",
    "    # Sentiment analysis could be added here with TextBlob or VADER\n",
    "    \n",
    "    return processed_texts, tfidf_matrix, tfidf_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the analysis.\"\"\"\n",
    "    print(\"Starting National Anthem Multimodal Analysis...\")\n",
    "    \n",
    "    # Part C: Textual Analysis\n",
    "    anthem_texts = load_anthem_texts()\n",
    "    processed_texts, tfidf_matrix, tfidf_df = analyze_anthem_texts(anthem_texts)\n",
    "\n",
    "    print(f\"\\nAnalysis complete. Results saved to {RESULTS_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
