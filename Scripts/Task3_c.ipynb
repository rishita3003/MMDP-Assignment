{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Part c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import librosa\n",
    "import librosa.display\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data folders\n",
    "ANTHEM_TEXT_DIR = \"national_anthems\"  # Folder containing anthem text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "RESULTS_DIR = \"anthem_analysis_results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "#################################################################\n",
    "# PART C: TEXTUAL ANALYSIS\n",
    "#################################################################\n",
    "\n",
    "def load_anthem_texts():\n",
    "    \"\"\"Load all anthem text files into a dictionary.\"\"\"\n",
    "    anthem_texts = {}\n",
    "    \n",
    "    for file_path in glob.glob(os.path.join(ANTHEM_TEXT_DIR, \"*.txt\")):\n",
    "        country_name = os.path.basename(file_path).replace('.txt', '')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                anthem_texts[country_name] = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(anthem_texts)} anthem text files.\")\n",
    "    return anthem_texts\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text: lowercase, remove punctuation, remove stopwords, lemmatize.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def analyze_anthem_texts(anthem_texts):\n",
    "    \"\"\"Perform textual analysis on anthem texts.\"\"\"\n",
    "    print(\"\\n--- TEXTUAL ANALYSIS ---\")\n",
    "    \n",
    "    # Preprocess texts\n",
    "    processed_texts = {country: preprocess_text(text) for country, text in anthem_texts.items()}\n",
    "    \n",
    "    # Basic statistics\n",
    "    text_lengths = {country: len(text.split()) for country, text in processed_texts.items()}\n",
    "    avg_length = np.mean(list(text_lengths.values()))\n",
    "    print(f\"Average anthem length (after preprocessing): {avg_length:.2f} words\")\n",
    "    \n",
    "    # Find shortest and longest anthems\n",
    "    shortest = min(text_lengths.items(), key=lambda x: x[1])\n",
    "    longest = max(text_lengths.items(), key=lambda x: x[1])\n",
    "    print(f\"Shortest anthem: {shortest[0]} ({shortest[1]} words)\")\n",
    "    print(f\"Longest anthem: {longest[0]} ({longest[1]} words)\")\n",
    "    \n",
    "    # Create a single corpus for overall analysis\n",
    "    all_text = ' '.join(processed_texts.values())\n",
    "    \n",
    "    # Most common words across all anthems\n",
    "    words = all_text.split()\n",
    "    word_counts = Counter(words)\n",
    "    print(\"\\nMost common words across all anthems:\")\n",
    "    for word, count in word_counts.most_common(15):\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    # Create word cloud for all anthems\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Most Common Words in National Anthems')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'anthem_wordcloud.png'))\n",
    "    \n",
    "    # TF-IDF Analysis\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform([text for text in processed_texts.values()])\n",
    "    \n",
    "    # Get feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Create DataFrame for easier analysis\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), \n",
    "                           index=list(processed_texts.keys()),\n",
    "                           columns=feature_names)\n",
    "    \n",
    "    # Save TF-IDF matrix\n",
    "    tfidf_df.to_csv(os.path.join(RESULTS_DIR, 'anthem_tfidf.csv'))\n",
    "    \n",
    "    # Dimensionality reduction for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "    # Create PCA plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)\n",
    "    \n",
    "    # Add country labels to points\n",
    "    for i, country in enumerate(processed_texts.keys()):\n",
    "        plt.annotate(country, (pca_result[i, 0], pca_result[i, 1]), \n",
    "                    fontsize=8, alpha=0.8)\n",
    "    \n",
    "    plt.title('PCA of National Anthem Texts')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'anthem_text_pca.png'))\n",
    "    \n",
    "    # Clustering analysis\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "    clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "    \n",
    "    # Create a DataFrame with clustering results\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'Country': list(processed_texts.keys()),\n",
    "        'Cluster': clusters\n",
    "    })\n",
    "    \n",
    "    # Print clusters\n",
    "    print(\"\\nAnthem text clusters:\")\n",
    "    for cluster_id in range(5):\n",
    "        countries = cluster_df[cluster_df['Cluster'] == cluster_id]['Country'].tolist()\n",
    "        print(f\"Cluster {cluster_id}: {', '.join(countries[:5])}{'...' if len(countries) > 5 else ''}\")\n",
    "    \n",
    "    # Save clustering results\n",
    "    cluster_df.to_csv(os.path.join(RESULTS_DIR, 'anthem_text_clusters.csv'), index=False)\n",
    "    \n",
    "    # Topic modeling with NMF\n",
    "    nmf = NMF(n_components=5, random_state=42)\n",
    "    nmf_result = nmf.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Get top words for each topic\n",
    "    print(\"\\nTop words for each topic:\")\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        top_words_idx = topic.argsort()[:-11:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "    \n",
    "    # Save topic modeling results\n",
    "    nmf_df = pd.DataFrame(nmf_result, index=list(processed_texts.keys()))\n",
    "    nmf_df.to_csv(os.path.join(RESULTS_DIR, 'anthem_topics.csv'))\n",
    "    \n",
    "    # Sentiment analysis could be added here with TextBlob or VADER\n",
    "    \n",
    "    return processed_texts, tfidf_matrix, tfidf_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the analysis.\"\"\"\n",
    "    print(\"Starting National Anthem Multimodal Analysis...\")\n",
    "    \n",
    "    # Part C: Textual Analysis\n",
    "    anthem_texts = load_anthem_texts()\n",
    "    processed_texts, tfidf_matrix, tfidf_df = analyze_anthem_texts(anthem_texts)\n",
    "\n",
    "    print(f\"\\nAnalysis complete. Results saved to {RESULTS_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
